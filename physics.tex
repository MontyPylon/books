\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{float}
\usepackage{setspace}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage[normalem]{ulem}
\usepackage{color}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{comment}
\usepackage{multirow}
\usepackage[makeroom]{cancel}
\usepackage{braket}

\newcommand\Tstrut{\rule{0pt}{5.6ex}}       % "top" strut
\newcommand{\dbar}{d\hspace*{-0.08em}\bar{}\hspace*{0.1em}}

\title{The Physics Book}
\author{Jeremy Cook}
\date{December 2017}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introduction}

\newpage
\section{Classical Dynamics}

\newpage
\section{Waves and Optics}

\newpage
\section{Classical Electrodynamics (TODO)}

\newpage
\section{Modern Physics (?)}

\newpage
\section{Quantum Mechanics}

\noindent \textbf{Schrodinger Equation} \\

\begin{equation}
    \frac{\partial \Psi}{\partial t} = \frac{i\hbar}{2m}\frac{\partial^2 \Psi}{\partial x^2} - \frac{i}{\hbar}V\Psi
\end{equation}

\noindent \textbf{Probability Density} \\
\begin{equation}
    \int_{-\infty}^{\infty} |\Psi (x,t)|^2 dx = 1
\end{equation}

\noindent \textbf{Time derivative of $\Psi$} \\

\begin{equation}
    \frac{\partial}{\partial t} |\Psi |^2 = \Psi^{*}\frac{\partial \Psi}{\partial t} + \frac{\partial \Psi^{*}}{\partial t}\Psi \label{t1}
\end{equation}

\noindent Given that the Schrodinger equation tells us

\begin{gather}
    \frac{\partial \Psi}{\partial t} = \frac{i\hbar}{2m}\frac{\partial^2 \Psi}{\partial x^2} - \frac{i}{\hbar}V\Psi \\
    \frac{\partial \Psi^{*}}{\partial t} = -\frac{i\hbar}{2m}\frac{\partial^2 \Psi^{*}}{\partial x^2} + \frac{i}{\hbar}V\Psi^{*}
\end{gather}

\noindent then we can substitute this in for \eqref{t1} and get

\begin{equation}
    \frac{\partial}{\partial t} |\Psi|^2 = \frac{i\hbar}{2m} \left ( \Psi^{*}\frac{\partial^2 \Psi}{\partial x^2} - \frac{\partial^2 \Psi^{*}}{\partial x^2}\Psi\right ) = \frac{\partial}{\partial x} \left [ \frac{i\hbar}{2m} \left ( \Psi^{*}\frac{\partial \Psi}{\partial x} - \frac{\partial \Psi^{*}}{\partial x}\Psi\right )\right ]
\end{equation}

\subsection{Momentum}
For a particle in state $\Psi$, the expectation value of $x$ is

\begin{gather*}
    \braket{x} = \int_{-\infty}^{\infty} x |\Psi(x,t)|^2 dx
\end{gather*}

The expectation value of the particle is not to be confused with the average position of the particle over time. Rather, $\braket{x}$ is the average of measurements performed on particles all in the state $\Psi$, which means that either you must find some way of returning the particle to its original state after each measurement, or else you have to prepare a whole ensemble of particles, each in the same state $\Psi$, and measure the positions of all of them: $\braket{x}$ is the average of these results.\\

\noindent \textbf{Operator} \\
\indent An operator is an instruction to do something to the function that follows it. In quantum mechanics most operators are derivatives and multipliers.

\begin{gather*}
    \braket{x} = \int \Psi^{*} (x) \Psi dx \\
    \braket{p} = \int \Psi^{*} \left ( \frac{\hbar}{i} \frac{\partial}{\partial x}\right ) \Psi dx
\end{gather*}

\noindent which means the $x$ and $p$ operators are

\begin{gather*}
    \hat{x}: f \mapsto xf \\
    \hat{p}: f \mapsto \frac{\hbar}{i} \frac{\partial f}{\partial x}.
\end{gather*}

\subsection{Connection Between Classical and Quantum}
All classical dynamical variables can be expressed in terms of position and momentum. All classical laws work, but on average.

\begin{gather*}
    v = \frac{dx}{dt} \implies \braket{v} = \frac{d\braket{x}}{dt} \\
    p = m\frac{dx}{dt} \implies \braket{p} = m \frac{d\braket{x}}{dt} \\
    \frac{dp}{dt} = -\frac{\partial V}{\partial x} \implies \frac{d\braket{p}}{dt} = \bigg \langle -\frac{\partial V}{\partial x} \bigg \rangle
\end{gather*}

\subsection{The Uncertainty Principle}
Quantitatively,

\begin{gather*}
    \sigma_{x} \sigma_{p} \geq \frac{\hbar}{2},
\end{gather*}

\noindent \textbf{DeBroglie Formula} \\
\begin{gather*}
    p = \hbar k
\end{gather*}

\noindent where $\sigma_{x}$ is the standard deviation in $x$, and $\sigma_{p}$ is the standard deviation in p.

\subsection{Time Independent Schrodinger Equation}
In order to solve the Schrodinger Equation, we can employ the method of separation of variables (the physicists first line of attack for any PDE). Therefore we split $\Psi(x,t)$ into

\begin{gather*}
    \Psi(x,t) = \psi(x)\phi(t).
\end{gather*}

\indent This is of course assuming that our potential $V(x)$ is independent of time, which it almost always is. From this we then find

\begin{gather*}
    \frac{\partial \Psi}{\partial t} = \psi \frac{d\phi}{dt}, \quad \frac{\partial^2 \Psi}{\partial x^2} = \frac{d^2 \psi}{dx^2} \phi
\end{gather*}

which when combined with the Schrodinger Equation and dividing by $\psi \phi$ gives us 

\begin{gather*}
   i\hbar \frac{1}{\phi} \frac{d\phi}{dt} = -\frac{\hbar^2}{2m} \frac{1}{\psi} \frac{d^2\psi}{dx^2} + V.
\end{gather*}

The left side is independent of $x$, and the right hand side is independent of $t$. From this we can deduce that both sides of the equation must be equal to the same constant, which we will call $E$. Separating the two halves of the equation gives us an ODE for $\psi(x)$ and an ODE for $\phi(t)$,

\begin{gather*}
    \frac{d \phi}{dt} = -\frac{iE}{\hbar}\phi
\end{gather*}
and
\begin{gather}
    -\frac{\hbar^2}{2m} \frac{d^2 \psi}{dx^2} + V\psi = E \psi,
    \label{TISE}
\end{gather}

\noindent where (\ref{TISE}) is the Time Independent Schrodinger Equation. Solutions to the Schrodinger equation using the separation of variables method are in states of definite total energy. In classical mechanics the total energy is called the Hamiltonian:

\begin{gather*}
    H(x,p) = \frac{p^2}{2m} + V(x).
\end{gather*}

The corresponding Hamiltonian operator is

\begin{gather*}
    \hat{H}: f \mapsto  -\frac{\hbar^2}{2m}\frac{\partial^2 f}{\partial x^2} + V(x).
\end{gather*}

We can then write the time-independent Schrodinger equation (\ref{TISE}) as

\begin{gather*}
    \hat{H} \psi = E \psi,
\end{gather*}

and the expectation value of the total energy is

\begin{gather*}
    \braket{H} = \int \psi^* \hat{H} \psi dx = E \int |\psi|^2 dx = E\int \Psi dx = E
\end{gather*}

\begin{gather*}
    \Psi(x,t) = \sum_{n=1}^{\infty} c_{n} \psi_{n}(x) e^{-iE_{n}t/\hbar} = \sum_{n=1}^{\infty} c_{n} \Psi_{n}(x,t)
\end{gather*}

\noindent \textbf{Fourier's Trick} \\

\begin{gather*}
    \int \psi_{m}^{*} f(x) dx = \sum_{n=1}^{\infty} c_{n} \int \psi_{m}^{*} \psi_{n}(x) dx = \sum_{n=1}^{\infty} c_{n} \delta_{mn} = c_{m}
\end{gather*}

\subsection{The Infinite Square Well}
Suppose

\begin{gather*}
    V(x) = \begin{cases} 
                      0, & \text{if}\ 0 \leq x \leq a \\
                      \infty, & \text{otherwise}.
                   \end{cases}
\end{gather*}


\noindent \textbf{Free Particle} \\
\indent If a particle is not under the influence of a potential function, i.e. $V(x) = 0$ everywhere, then we have a free particle. The general solution to the time-independent Schrodinger equation then reads

\begin{gather*}
    \psi(x) = Ae^{ikx} + Be^{-ikx},
\end{gather*}

\noindent and the time-dependent solution is found by tacking on the time dependence $\exp{-iEt/\hbar}$, as usual:

\begin{gather*}
    \Psi(x,t) = Ae^{ik(x - \hbar k t / 2m)} + Be^{-ik(x - \hbar k t / 2m)}.
\end{gather*}

\indent Any function of $x$ and $t$ that depends on these variables in the special combination $(x \pm vt)$ represents a wave of fixed profile. Since every point on the waveform is moving along with the same velocity, its shape doesn't change as it propagates. The speed of these waves is 

\begin{gather*}
    v_{\text{quantum}} = \frac{\hbar |k|}{2m} = \sqrt{\frac{E}{2m}} = \frac{v_{\text{classical}}}{2}.
\end{gather*}

\indent The general solution to the time-dependent Schrodinger equation is a linear combination of separable solutions, except integrated over a continuous variable $k$ instead of a sum over a discrete index $n$:

\begin{gather*}
    \Psi(x,t) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \phi(k) \exp{\left (ikx - \frac{i\hbar k^2 t}{2m} \right )} dk.
\end{gather*}

\begin{gather*}
    f(x) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} F(k) e^{ikx} \Longleftrightarrow F(k) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} f(x) e^{-ikx}
\end{gather*}

\subsection{Quantum Harmonic Oscillator}
Consider the potential of a harmonic oscillator

\begin{gather*}
    V(x) = \frac{1}{2} k x^2.
\end{gather*}

The quantum potential can then be written in terms of $m$ and $\omega$ where $w^2 = k/m$. The potential is then

\begin{gather*}
    V(x) = \frac{1}{2} m \omega^2 x^2.
\end{gather*}

The significance of this potential is that it is a parabolic approximation to any minimum in any potential. We can see this by expanding a Taylor series about the minimum at $x_{0}$ of a potential:

\begin{gather*}
    V(x) = V(x_{0}) + V'(x_{0})(x - x_{0}) + \frac{1}{2}V''(x_{0})(x-x_{0})^2 + ...\, ,
\end{gather*}

subtract $V(x_{0})$ (you can add a constant to $V(x)$ with impunity, since that doesn't change the force), recognize that $V'(x_{0}) = 0$ since $x_{0}$ is a minimum, and drop the higher order terms (which are negligible as long as $(x - x_{0})$ stays small), we get

\begin{gather*}
    V(x) \approx \frac{1}{2} V''(x_{0}) (x-x_{0}),
\end{gather*}

which describes simple harmonic oscillation. Plugging our harmonic oscillator potential into the time-independent Schrodinger equation gives

\begin{gather*}
    -\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2} + \frac{1}{2} m \omega^2 x^2 \psi = E \psi.
\end{gather*}

Define the ladder operator $a$:

\begin{gather*}
    a_{\pm} \equiv \frac{1}{\sqrt{2 \hbar m \omega}} \left ( \mp ip + m\omega x\right ) 
\end{gather*}

We know that operators commute with any constant.

\begin{gather*}
    \hat{H}(a_{+} \psi ) = (E + \hbar \omega) (a_{+}\psi ) \\
    \hat{H}(a_{-} \psi ) = (E - \hbar \omega) (a_{-}\psi ) \\
\end{gather*}

Theorem: The total energy $E$ must exceed the minimum value of $V(x)$ for there to exist a normalizable solution to the time-independent Schrodinger equation. 

\begin{gather*}
    \psi_{n}(x) = A_{n} (a_{+})^{n} \psi_{0}(x) \quad \quad E_{n} = \left ( n + \frac{1}{2} \right ) \hbar \omega
\end{gather*}

\begin{gather*}
    \psi_{n} = \frac{1}{\sqrt{n!}}(a_{+})^{n} \psi_{0}
\end{gather*}

\begin{gather*}
    \int_{-\infty}^{\infty} \psi_{m}^{*}\psi_{n}dx = \delta_{m,n}
\end{gather*}

Orthonormality means that we can again use Fourier's trick to evaluate the coefficients, when we expand $\Psi(x,0)$ as a linear combination of stationary states, and $|c_{n}|^2$ is again the probability that a measurement of the energy would yield the value $E_{n}$.

\subsection{Linear Algebra} 
A \textbf{vector space} $V$ consists of \textbf{vectors} $(\ket{\alpha}, \ket{\beta}, \ket{\gamma},...)$ together with a set of \textbf{scalars} $(a,b,c,...)$, which is \textbf{closed} under vector addition and scalar multiplication. \\

\noindent \textbf{Vector Addition:}
\begin{itemize}
    \item[(i)] Closed: Take $\ket{\alpha},\ket{\beta} \in V$, then $\ket{\alpha} + \ket{\beta} = \ket{\gamma}$ s.t. $\ket{\gamma} \in V$.
    \item[(ii)] Commutative: $\ket{\alpha} + \ket{\beta} = \ket{\beta} + \ket{\alpha}$.
    \item[(iii)] Associative: $\ket{\alpha} + (\ket{\beta} + \ket{\gamma}) = (\ket{\alpha} + \ket{\beta}) + \ket{\gamma}$.
    \item[(iv)] Zero element: $\ket{\alpha} + \ket{0} = \ket{\alpha}$.
    \item[(v)] Additive inverse: $\ket{\alpha} + \ket{-\alpha} = \ket{0}$.
\end{itemize}

\noindent \textbf{Scalar Multiplication:}
\begin{itemize}
    \item[(i)] Closed: Take $\ket{\alpha} \in V$, then $a\ket{alpha} = \ket{\gamma}$ s.t. $\ket{\gamma} \in V$.
    \item[(ii)] Distributive with respect to vector addition: $a(\ket{\alpha} + \ket{\beta}) = a\ket{\alpha} + a\ket{\beta}$.
    \item[(ii)] Distributive with respect to scalar multiplication: $(a + b)\ket{\alpha} = a\ket{\alpha} + b\ket{\alpha}$.
    \item[(iii)] Associative with respect to multiplication of scalars: $a(b\ket{\alpha}) = (ab)\ket{\alpha}$.
    \item[(iv)] Zero: $0\ket{\alpha} = \ket{0}$.
    \item[(v)] Unity: $1 \ket{\alpha} = \ket{\alpha}$.
\end{itemize}

A vector $\gamma$ is said to be \textbf{linearly independent} of a set of vectors if it cannot be written as a linear combination of them. By extension, a set of vectors is ``linearly independent'' if each one is linearly independent of all the rest. A collection of vectors is said to \textbf{span} the space if every vector can be written as a linear combination of the members of this set. A set of \textit{linearly independent} vectors that spans the space is called a \textbf{basis} The number of vectors in any basis is called the \textbf{dimension} of the space. 
\\

Proof by contradiction that the components of a vector with respect to a given basis are \textit{unique}: Suppose $\ket{\alpha} = a_{1}\ket{e_{1}} + a_{2}\ket{e_{2}} + ... + a_{n}\ket{e_{n}}$ and $\ket{\alpha} = b_{1}\ket{e_{1}} + b_{2}\ket{e_{2}} + ... + b_{n}\ket{e_{n}}$. Subtract these to get $0 = (a_{1} - b_{1})\ket{e_{1}} + (a_{2} - b_{2})\ket{e_{2}} + ... + (a_{n} - b_{n})\ket{e_{n}}$. The coefficients must differ by at least on term, take $a_{j} \neq b_{j}$ and then divide by this term, and isolate $\ket{e_{j}}$ to get:

\begin{gather*}
    \ket{e_{j}} = -\frac{(a_{1} - b_{1})}{(a_{j} - b_{j})}\ket{e_{1}} - \frac{(a_{2} - b_{2})}{(a_{j} - b_{j})}\ket{e_{2}} - ... - 0\ket{e_{j}} - ... - \frac{(a_{n} - b_{n})}{(a_{j} - b_{j})}\ket{e_{n}}    
\end{gather*}

However since our basis is by definition linearly independent, it must be that $a_{i} = b_{i}\ \forall i$. $\qed$ \\

\noindent \textbf{Inner Products} \\
\indent The cross product in Euclidean spaces does not generalize in any natural way to $n$-dimensional vector spaces, but the dot product does, and we call it the \textbf{inner product}. The inner product of two vectors is a complex number, which we write as $\braket{\alpha}{\beta}$, with the following properties:

\begin{gather*}
    \braket{\beta|\alpha} = \braket{\alpha|\beta}^{*}, \\
    \braket{\alpha|\alpha} \geq 0, \quad \text{and} \braket{\alpha|\alpha} = 0 \Leftrightarrow \ket{\alpha} = \ket{0}, \\
    \bra{\alpha}(b\ket{\beta} + c\ket{\gamma}) = b\braket{\alpha|\beta} + c\braket{\alpha|\gamma}.
\end{gather*}

A vector space with an inner product is called an \textbf{inner product space}. The \textbf{norm} of a vector is 

\begin{gather*}
    \lVert \alpha \rVert \equiv \sqrt{ \braket{\alpha|\alpha} }
\end{gather*}

\noindent \textbf{Schwartz inequality} \\
\begin{gather*}
    \lvert \braket{\alpha|\beta} \rvert^{2} \leq \braket{\alpha|\alpha} \braket{\beta|\beta}
\end{gather*}

Proof: Taking the vector $\ket{\gamma}$ to be

\begin{gather*}
    \ket{\gamma} = \ket{\beta} - \frac{\braket{\alpha|\beta}}{\braket{\alpha|\alpha}}\ket{\alpha}
\end{gather*}

we can then see that 

\begin{gather*}
    \braket{\gamma|\gamma} = \bra{\gamma} \left (\ket{\beta} - \frac{\braket{\alpha|\beta}}{\braket{\alpha|\alpha}}\ket{\alpha} \right ) =  \braket{\gamma|\beta} - \frac{\braket{\alpha|\beta}}{\braket{\alpha|\alpha}}\braket{\gamma|\alpha} \geq 0\\
    \braket{\gamma|\beta}^{*} = \braket{\beta|\gamma} = \bra{\beta} \left ( \ket{\beta} - \frac{\braket{\alpha|\beta}}{\braket{\alpha|\alpha}}\ket{\alpha}\right ) = \braket{\beta|\beta} - \frac{\lvert \braket{\alpha|\beta} \rvert^{2}}{\braket{\alpha|\alpha}},\ \text{which is \textit{real}}. \\
    \braket{\gamma|\alpha}^{*} = \braket{\alpha|\gamma} = \bra{\alpha} \left ( \ket{\beta} - \frac{\braket{\alpha|\beta}}{\braket{\alpha|\alpha}}\ket{\alpha}\right ) = \braket{\alpha|\beta} - \braket{\beta|\alpha} = 0. \braket{\gamma|\alpha} = 0. \\
    \braket{\gamma|\gamma} = \braket{\beta|\beta} - \frac{\lvert \braket{\alpha|\beta} \rvert^{2}}{\braket{\alpha|\alpha}} \geq 0 \implies 
    \lvert \braket{\alpha|\beta} \rvert^{2} \leq \braket{\alpha|\alpha} \braket{\beta|\beta}. \qed
\end{gather*}

\noindent \textbf{Triangle Inequality} \\
\begin{gather*}
    \lVert (\ket{\alpha} + \ket{\beta}) \rVert \leq \lVert \alpha \rVert + \lVert \beta \rVert
\end{gather*}

\newpage
\section{Thermodynamics}

\subsection{Constants}
\noindent \textbf{Boltzmann's Constant} \quad $k_{b} = 1.38064852 \times 10^{-23}\ m^2 \cdot kg \cdot s^{-2} \cdot K^{-1}$\\
\noindent \textbf{Avogadro's Number} \quad $N_{A} = 6.023 \times 10^{23}$ \\
\noindent TODO add all in table


\subsection{Microstates, Macrostates, Multiplicity}
Irreversible processes are not \textit{inevitable}, they are just overwhelmingly \textit{probable}. Each of the possible outcomes of an event is called a \textbf{microstate}. If we only state properties of the outcome, and not it's specific state, we call it a \textbf{macrostate}. As an example, let us flip a coin 3 times. A microstate would be the outcome $(H,T,T)$, and a macrostate would be the outcomes that contains 2 tails. The number of microstates corresponding to a given macrostate is called the multiplicity of that macrostate. The symbol used for multiplicity in this book is $\Omega(n)$, for example, $\Omega(2\ \text{tails})$ for 3 coin flips is 3. 

\subsection{Ideal Gas Law}
\indent The ideal gas law relates pressure, volume, and temperature for ideal gasses. The conditions for an ideal gas are (i) randomly moving point particles (ii) purely elastic collisions (iii) extremely rare collisions. Below is the formula, where $n$ is the number of moles, $N$ is the number of molecules, i.e. $n \cdot N_{A} = N$, and $k_{b} = N_{A} / R$, then
\begin{gather*}
    PV = nRT \quad \text{or} \quad PV = Nk_{b}T
\end{gather*}

\subsection{Equipartition Theorem} 
\indent At temperature T (K), the average energy of any quadratic degree of freedom is $\frac{1}{2}kT$. Some examples of quadratic degrees of freedom include

\begin{gather*}
    \frac{1}{2}mv_{x}^{2},\ \frac{1}{2}mv_{y}^2,\ \frac{1}{2}mv_{z}^2,\ \frac{1}{2}I\omega_{x}^2,\ \frac{1}{2}I\omega_{y}^2,\ \frac{1}{2}k_{s}x^2,
\end{gather*}

\noindent etc. If a system contains $N$ molecules, each with degrees of freedom, and there are no other (non-quadratic) temperature dependent forms of energy, then its \textit{total} thermal energy is

\begin{gather*}
    U_{\text{thermal}} = N f\frac{1}{2}k T.
\end{gather*}

\indent For a monatomic molecule, $f = 3$ for the three translational degrees of freedom. A diatomic molecule has an additional 2 degrees of freedom for rotation about two axes, and is symmetric about 1 axis, which does not contribute to the total thermal energy. Therefore for a diatomic molecule, $f = 5$. For a solid $f=6$ for 3 degrees of freedom in spring kinetic energy, and 3 in spring potential. \\


\subsection{First Law of Thermodynamics}
\indent The law of conservation of energy states that the total energy of an isolated system is constant; energy can be transformed from one form to another, but can be neither created nor destroyed. The first law is often formulated (chemistry convention for work)

\begin{gather*}
    \Delta U = Q + W
\end{gather*}

\noindent where $\Delta U$ is the change in internal energy, $Q$ is the heat supplied to the system, and $W$ is the amount of work done on the system by its surroundings. The law can also be written as

\begin{gather*}
    dU = \dbar Q + \dbar W + \mu \, dN
\end{gather*}

\noindent where $\dbar$ represents a differential which is not exact. Therefore $\dbar Q$ and $\dbar W$ both depend on path, whereas $dU$ is an exact differential and is independent of path. $\dbar Q$ is the heat added to the system, while $\dbar W$ is the work done on the system and is equal to (chemistry convention)

\begin{gather*}
    \dbar W = -P\, dV
\end{gather*}

\noindent \textbf{Temperature} \\
\indent Temperature is fundamentally the measure of an objects tendency to spontaneously give up energy. This is not the definition of temperature, but merely a statement about temperature which happens to be true. \\

\noindent \textbf{Heat} \\
\indent Heat is defined as any spontaneous flow of energy from one object to another caused by a difference in temperature between the objects. \\

\noindent \textbf{Work} \\
\indent Work in thermodynamics is defined as any other transfer of energy into or out of a system. You do work on a system whenever you push on a piston, stir a cup of coffee, or run current through a resistor. 
\\

\noindent \textbf{Enthalpy} \\
\indent Assuming a constant pressure process, enthalpy is a measure of the total energy content of a system $U$ plus the work needed to make room for it, i.e. the external pressure on the system times the total volume of the system. 
\begin{gather*}
    H \equiv U + PV
\end{gather*}

It is a measure of the \textit{total} energy one would have to come up with to create a system out of nothing and put it into an environment with pressure $P$. When put in terms of $Q$, we see that the compression-expansion work cancels and only heat and other forms of work will change the enthalpy of the system. If no ``other'' types of work are being done, then enthalpy tells you directly how much heat has been added to the system. For this reason, we use enthalpy to define heat capacity.
\\

\noindent \textbf{Heat Capacity}\\
\indent The \textbf{heat capacity} of an object is the amount of heat needed to raise its temperature, per degree temperature increase:

\begin{gather*}
    C \equiv \frac{Q}{\Delta T}
\end{gather*}

\indent The \textbf{specific heat capacity} is an intensive property of a material and is defined as the heat capacity per unit mass:

\begin{gather*}
    c \equiv \frac{C}{m}
\end{gather*}

\indent Heat capacity at constant volume:

\begin{gather*}
    C_{V} = \left ( \frac{\partial U}{\partial T} \right )_{V} = \frac{Nfk_{b}}{2}
\end{gather*}

\indent Heat capacity at constant pressure:

\begin{gather*}
    C_{P} = \left ( \frac{\partial U}{\partial T}\right )_{P} + P\left ( \frac{\partial V}{\partial T}\right )_{P} = \left ( \frac{\partial H}{\partial T} \right )_{P}
\end{gather*}

\noindent \textbf{Black-body Radiation} \\
\indent Black-body radiation is the thermal electromagnetic radiation within or surrounding a body in thermodynamic equilibrium with its environment, or emitted by a black body (an opaque and non-reflective body). It has a specific spectrum and intensity that depends only on the body's temperature, which is assumed for the sake of calculations and theory to be uniform and constant.
\\

\noindent \textbf{Thermodynamic Identity} \\

\begin{gather*}
    dU = T\, dS - P\, dV + \mu \, dN
\end{gather*}

\begin{table}[H]
  \begin{center}
    \begin{tabular}{l|l|l|l} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \begin{tabular}{@{}l@{}}Types of \\ interaction\end{tabular} & \begin{tabular}{@{}l@{}}Exchanged \\ quantity\end{tabular} & \begin{tabular}{@{}l@{}}Governing \\ variable\end{tabular} & Formula \\
      \hline
      Thermal & energy & temperature & $\cfrac{1}{T} = \left ( \cfrac{\partial S}{\partial U} \right )_{V,N}$ \Tstrut\\ % top strut only
      mechanical & volume & pressure & $\cfrac{P}{T} = \left ( \cfrac{\partial S}{\partial V} \right )_{U,N}$ \Tstrut\\
      diffusive & particles & chemical potential & $\cfrac{\mu}{T} = - \left ( \cfrac{\partial s}{\partial N} \right )_{U,V}$ \Tstrut\\
    \end{tabular}
  \end{center}
\end{table}

\noindent \textbf{Stirling Approximation} \\
\indent For $N \gg 1$, the Stirling approximation works very well for calculating factorials,
\begin{gather*}
    N! \approx  N^N e^{-N} \sqrt{2\pi N},
\end{gather*}

\noindent and if you take the logarithm of both sides you have

\begin{gather*}
    \ln{N!} \approx N\ln{N} - N.
\end{gather*}

\noindent \textbf{Fundamental Assumption of Statistical Mechanics} \\
\indent In an isolated system in thermal equilibrium, all accessible microstates are equally probable. \\



\noindent \textbf{Entropy} \\

\begin{gather*}
    S \equiv k \ln{\Omega}
\end{gather*}


\subsection{Second Law of Thermodynamics} 
\indent The second law of thermodynamics states that the total entropy can never decrease over time for an isolated system, that is, a system in which neither energy nor matter can enter nor leave.
\\


\noindent \textbf{The Two State Paramagnet} \\
\indent A paramagnet is a material in which the constituent particle's spins ten to align parallel to any externally applied magnetic field. If the particles interact strongly enough with each other, then we call it a ferromagnet. We can refer to the individual magneticl particles as dipoles because each has its own magnetic dipole moment vector. In practice the dipole could be an electron, a group of electrons, or an atomic nucleus. If the dipole can only take on two discrete values, which we call a \textbf{two-state paramagnet}, either parallel or antiparallel to the external magnetic field.

\end{document}